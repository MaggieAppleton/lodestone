import type { TestCase } from "./types";

export const chatbotsTest: TestCase = {
	id: 1,
	name: "Chatbots",
	text: `Chatbots are not the ideal, ultimate interface for language models. Language models are a general tool that we'll use for specialised tasks. Each task will need its own specialised interface. We'll get much more leverage from agents in shared workspaces with shared context. Not all problems can be solved by text-based conversation. Suggesting so presumes all human problems are conversational and linguistic. Chatbots are not the optimal, universal user interface to language models. Chatbots are the lazy, catch-all choice for language model interfaces. We need sophisticated visual, spatial interfaces for large language models that beyond na√Øve text-based prompting. AI agents as special teammates in a multiplayer workspace. Chatbots are a decent interface for prototyping undefined workflows. At this point, I assume everyone knows why chatbots are flawed. Or at least realised they're not the optimal interface, like some people proclaimed in the beginning. It turns out text is not the universal interfaces, and it's not all we need. But I always assume things that are obvious to me are obvious to others. And it turns out they're not. And I'm surprised. I bet if you asked most people you know to explain to you why chatbots are not the ultimate interfaces to language models, they wouldn't have a quick, easy answer. It still feels worth explaining this to yourself.You can change the framing of it from a classic "chatbots are bad" to "chatbots were a necessary historical phase for us to move through, and we're finally moving beyond it. Or rather, chatbots are good for some tasks and workflows, but not all tasks and flows. The problem of open-ended text input boxes and language models. The problem with chatbots is primarily a problem with open-ended inputs. The major interface challenge of language models right now is that everyone is obsessed with the open-ended input box. This is framed as either a chat box or a search input, where users can type in anything they like. The problem is that users don't know what to type in, and there are definitely better and worse things to type in. Before language models, if a user saw a search input, they could pretty well guess that what they would get in return was a list of results related to their query. That's no longer true with language models. Now the user has no idea what they're going to get in return. It could be a rich table, it could be long paragraphs of information, it could be generated code that runs as an app, it could be a dynamic UI that displays their results in different ways. It's like a blank room. You walk into a blank room with no furniture in it to interact with, no signs on the walls telling you what to do, no hallways or divisions to suggest where you should go, no vibe to suggest what kind of activities happen here, no aesthetics that tell you this room is for acting professional or formal or laid back or creative. Guiding the user to type in something that will lead to useful results is an enormous interface challenge. At the moment, most of us are doing this with examples and written guidance. But I don't think that's actually enough. You have to teach the user how to prompt this app well. Complex information tasks for specific domains require specialised interfaces to support those complex tasks. Think academic research, accounting, insurance, investing, surgery, hospital care, dentistry, urban planning, election oversight, risk assessment, architecture, construction management, school administration. This doesn't mean the interface has to be complex, but some levels of complexity cannot be avoided. Limitations of chatbots, in their current form: I can't directly manipulate their outputs. I can only copy them, or give them a thumbs up or thumbs down. Or put them elsewhere to edit them, but then it's out of the reasoning loop. I especially feel this with Claude (which I use much more than ChatGPT).  This particularly applies to Artefacts. I'm usually generating a product/technical plan for what to do, and then some initial code. But sometimes two small points in a long technical plan aren't right, and I can't just go in and point at it and say "remove this", or directly delete it myself. Similarly, Claude often asks me four questions, then goes on to generate a bunch of code, but each of these questions needs to be answered in turn, and may need something specific in the code to change because of the answer. At the moment we do this with big batches of reasoning where details frequently get lost, and Claude ends up making stupid mistakes or introducing bugs in the code, or simply can't handle that volume of code and starts adding filler comments like // code remains the same here. Keeping track of previous versions is difficult. I'm personally carrying the mental load of all the questions and answers in the conversations so far, and having to keep track of which ones we have / haven't answered, which ones we've built solutions for, etc. The main thing here is this is a generic interface, and I want it to do a specific job: help me plan technical features or products, and work through the complexities of that plan, and write (most of) the code for it, advise me on the trade off of X library vs Y, help me debug specific issues that come up while writing the code."`,
};
